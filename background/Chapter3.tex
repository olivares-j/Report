%!TEX root = ../thesis.tex
\chapter{Bayesian formalism}
\label{chap:BHM}
This chapter provides a general introduction to probability theory and its application to parametric inference. Since the objective of this work is to infer the probability distributions of the cluster properties (e.g. luminosity and velocity), I give reason to prove that the Bayes' theorem provides the proper framework for the inference of the parameters governing these distributions. Later in this chapter, I describe the reason for which the Bayesian Hierarchical Models are the best option to parametric inference under the Bayesian framework.

In the following Sections I will describe in detail the assumptions I made to model the data and to select the prior distributions. The two final Sections of this Chapter focus on: the practical issues related to the sampling of the cluster distributions, and the description of details and assumptions embed in the codes I developed.

\section{Introduction to probability theory.}
 
Uncertainty and probability are closely entangled. Anything we measure has an associated uncertainty, otherwise is not a complete measurement \footnote{Upper and lower limits are examples of incomplete measurements.}. The term uncertainty must not be confused with the term error, which refers to the difference between the measured value of the quantity and the \emph{true} value\footnote{The true value is that which ideally results when the uncertainty tends to zero.} of it \citep{GUM2008}. It is commonly agreed that uncertainty of a measurement can be expressed in a probabilistic basis \citep{GUM2008}. This means that whenever we measure a quantity, lets say $a$, the distribution of the repeated measurements of $a$, is a probability distribution function, $p(a)$. As any other probability distribution, $p(a)$ satisfies the following properties:

\begin{enumerate}[label=\textbf{Property \arabic*}]
\item  It has units, those of the inverse of $a$. \label{property:1}
\item $p(a) \geq 0$. \label{property:3}
\item $1=\int_a p(a) da$. \label{property:3}
\end{enumerate}

These properties hold regardless of the dimension of $a$, it means that the joint uncertainty of all measured quantities of an object is also a probability distribution. Furthermore, they also hold if the probability distribution is conditioned in any other quantity. Lets imagine that we measure the positions, projected in the plane of sky (the plane perpendicular to the line of sight), of one star, these measurements are conditioned in the magnitude (brightness) of the object we measure. If the object is too bright, like the sun, it will saturate the detector and it will render the measurement useless. On the other hand, if the object is too faint we simple will not have enough photons to measure it. So, the stellar positions in the sky, which we can call $a$ and $b$ because they are two dimensions, are conditioned on the magnitude, $c$, of the object. Therefore, $p(a,b|c)$ must also satisfy:

\begin{itemize}
\item It has units of $a^{-1} b^{-1}$.
\item $p(a,b|c)\geq0$.
\item $1=\int_a \int_b p(a,b|c)da\cdot db$.
\end{itemize}

The link between joint and conditioned probabilities is given by the following symmetric definition:

\begin{align}
p(a,b)=p(a|b)\cdot p(b).\nonumber \\
p(a,b)=p(b|a) \cdot p(a).
\end{align}

This can be further conditioned on $c$ to obtain:
\begin{align}
\label{eq:conditioned}
p(a,b|c)=p(a|b,c)\cdot p(b|c),\nonumber \\
p(a,b|c)=p(b|a,c) \cdot p(a|c),
\end{align}

If the joint probability of $a$ and $b$ can be factorised, this is
\begin{align}
p(a,b)=p(a)\cdot p(b),\nonumber \\
p(a,b)=p(b) \cdot p(a),
\end{align}
then $a$ and $b$ are say to be \emph{independent}. An alternative option is to say that $a$ and $b$ are \emph{independent}, if the conditional probability of $a$ on $b$ is $p(a|b)=p(a)$.

The most important thing we can do with probability distributions is to integrate them. \ref{property:3} establish that the amount\footnote{Which could be infinite, like in Dirac's delta.} of probability $p(a)$ spread over the volume of the support of $a$ adds to one. This Property allows us to \emph{marginalise} any non-desired variable. Lets imagine again that $a$ and $b$ are the measured positions of some star and we have several measurements of these positions. Then we will have the joint probability distribution of them, $p(a,b)$ (must likely it will be a bivariate gaussian but that does not matter now). If we are interested lets say in the mean value of $a$, we first must get rid of $b$. For it, we \emph{marginalise} out $b$ in the following way,
\begin{align}
\label{eq:marginalisation}
p(a)=\int_b p(a,b)\cdot db.
\end{align}

Then we compute the \emph{expected value} of $a$, $E(a)$, which is identified with the mean of $a$ once we have drawn many realisations from its probability distribution. To compute it, we add all the possible values of $a$ weighted by their probability. This is,

\begin{align}
\label{eq:expectation}
E(a)=\int_a a\cdot p(a)\cdot da.
\end{align}

Once again, these last two equations (\ref{eq:marginalisation} and \ref{eq:expectation}) hold in case they are conditioned in any other measurement. For example, the magnitude of the object, as in our previous analogy. Notice however that, once the brightness of the object lay within in the dynamic range of the detector, $a$ and $b$ became \emph{independent} of the magnitude.

It is important to recall that the term measurement, and its unavoidable uncertainty, refer not just to directly measured quantities, like the photons (counts) and pixels in a CCD, but also to indirect measurements. Stellar magnitudes and positions in the sky, for example, are indirect measurements derived from the direct measurement of photons, pixels and telescope arrangement. This generalisation also applies to the measurement of parameters in any physical or statistical model, like the one I will describe in the following Section.

\subsection{Bayes theorem}
The definition of conditioned probability (Eq. \ref{eq:conditioned}) leads to the Bayes' theorem:
\begin{equation}
p(a|b,c) = \frac{p(b|a,c)\cdot p(a|c)}{p(b|c)}.
\end{equation}
Integrating on $a$ we find that,
\begin{align}
\label{eq:evidence}
p(b|c) \cdot \int_a p(a|b,c)\cdot da = \int_a p(b|a,c) \cdot p(a|c) \cdot da \nonumber \\
Z \equiv p(b|c) = \int_a p(b|a,c) \cdot p(a|c) \cdot da.
\end{align}
In this last equation $Z$ refers to what is known as the \emph{evidence}. I will come back to this \emph{evidence} in a few paragraphs. This last Eq. also illustrates that $p(b|c)$ is a normalisation constant which can be evaluated once $p(b|a,c)$ and $p(a|c)$ are known. These two terms are commonly referred as the \emph{likelihood}  ($p(b|a,c)$), and the \emph{prior} ($p(a|c)$). Also the term the term $p(a|b,c)$ is called the \emph{posterior}. These names arise in the context of parametric inference, as I will describe in the next paragraph. However, it worths mention that, although formally the likelihood and the prior are probability distributions in $b$ and $a$, respectively, for $p(a|b,c)$ to be a probability distribution on $a$, it only suffices that the product of the likelihood times the prior does not vanish everywhere or be negative anywhere\footnote{Although negative probabilities may have sense in quantum mechanics. See for example \citet{1942RSPSA.180....1D}}. In this case, they are called \emph{improper} priors or likelihoods. If their product vanishes everywhere, which may be the case if the prior is terribly specified or if the likelihood does not take proper account of extreme data, then the posterior is not a probability distribution due to a division by zero. In any case, it makes no sense try to estimate the parameters of a model with zero evidence.

\subsubsection{Models and parametric inference}
In a broad sense, models are representation or abstraction of the knowledge about something. Sometimes the knowledge is shared by others, some time it is not. They are everywhere in our daily life: from the words we spoke every day, to the evolution of the species and the general relativity; from a kid's draw to the cosmological models. In science, however, we restrict the concept of model to a mathematical representation of the relations among the variables. If the model is parametric, the variables include the data $\mathbf{D}$, which the model attempts to model, and the parameters $\mathbf{\theta}$. Parameters are free variables that allow the model to describe the data. Thus a parametric model $\mathcal{M}$, can be represented as:

\begin{equation}
\label{eq:model}
\mathcal{M}=\left\{ f(\mathbf{D}| \mathbf{\theta}), \ \ \theta \in \Theta\right\},
\end{equation}
 where $f(|)$ is the function that relates data and parameters, and $\mathbf{\theta} \subset \mathbb{R}^k$  with $k$ the dimension of $\mathbf{\theta}$.
Parametric inference is then the act of finding the distribution of $\mathbf{\theta}$ given the data $\mathbf{D}$.
 
The Bayes' theorem allows to perform parametric inference, which is called then Bayesian inference. In this context the Bayes'  theorem is:
\begin{equation}
p(\mathbf{\theta}|\mathbf{D},M) = \frac{p(\mathbf{D}|\mathbf{\theta},M)\cdot p(\mathbf{\theta}|M)}{p(\mathbf{D}|M)}.
\end{equation}
where $\mathbf{\theta},\mathbf{D}$ and $M$ are correspond, respectively, to the parameters in the model, the data which the model tries to describe, and the prior information used in the construction of the model. 
Whenever we have a model, we have prior knowledge over it. Actually, it can be classified in two kinds of prior information. One refers to the prior information conveyed in the model, which I call $M$. This is the information that the creator of the model uses to establish the relations among the elements of the model: variables. The second kind of prior, $p(\mathbf{\theta}|M)$ refers to the statement the user of the model made of his/her believes about the probability distribution of the parameter values. This is indeed subjective. However, it is, in my opinion, less subjective than the former, $M$, prior information. At least in this last kind, the subjectivity is expressed objectively in a probabilistic, and therefore measurable way.

The likelihood of the data $p(\mathbf{D}|\mathbf{\theta},M)$, is a probability distribution on the data, $\mathbf{D}$. However, it is a function on the parameters, $\mathbf{\theta}$, which corresponds to the function $f$ of Eq. \ref{eq:model}. If we assume that data is independent from each other, which means that the probability of measuring the value of one datum is independent of the measured value of another datum, then, the joint probability  $p(\mathbf{D}|\mathbf{\theta},M)$ can be expressed as,

\begin{equation}
 p(\mathbf{D}|\mathbf{\theta},M) = \prod_{n=1}^N p(d_n|\mathbf{\theta},M), \ \ n=\{1,2,...,N\}
\end{equation}

The term $p(d_n|\mathbf{\theta},M)$ is the likelihood of datum $d_n$. This term also as the \emph{generative} model of the data since it contains the necessary information to generate the data\footnote{Actually the \emph{true} data. To generate the observed data the noise process must be also specified.}.

I interpret the Bayes' theorem, as the probabilistic way to update knowledge. It is the way, to update knowledge once we recognise the uncertainty associated to it. In my perspective, knowledge is always uncertain, even if its uncertainty is negligible given the current evidence that supports it. The Bayes' theorem helps us to update our prior believes by means of the data, once we multiply them by the likelihood of it. Then, the posterior probabilities of the parameters given the current data, became the new prior believes once more data is available. Furthermore, the Bayes' theorem also provides the objective way to compare two models and update the prior information, $M$, used to construct them. This is called model selection.

\subsection{Model Selection}

Whenever we have a data set and two or more models that attempt to describe it, the most straightforward thing to do is to compare these models. Almost always, we want to select the \emph{best} model. Obviously the term \emph{best} depends on the objective of research. For example, lets imagine that our data set consists of a set of measurements of the positions of an object as it moves in the sky. If we were interested in reproducing exactly the same points in the data set, the \emph{best} model will be a polynomial with degree equal to the number of points. This polynomial will pass trough all the points. 

Once we recognise the unavoidable uncertainty of the data, we realise that an exact representation of the data is of poor use. 
In general, we are interested in the predictive capabilities of a model, this is its ability to predict future observations rather than to replicate the ones we have. Thus, an exact representation of the observed data (an over-fitted model), will poorly describe any new data set. In this sense, an over-fitted model \emph{memorises} the data rather than \emph{learns} from them.

A model that \emph{learns} from the data is that which obtains the \emph{true} underlying relation embedded in data.  This \emph{true} underlying relation produces de data, once it is convolved (added) with the source of uncertainty. Thus, we call \emph{deconvolution} the the process by which the \emph{true} underlying relation is obtained. Nevertheless, we still need to select among learning models.  

We can draw some help from the commonly known Ockham's razor or principle \footnote{The origin of this motto and its exact phrasing is beyond the scope of this work. I just mention that paradoxically, an ancient formulation is attributed to Ptolomey: "We consider it a good principle to explain the phenomena by the simplest hypothesis possible" \citep{Franklin2001}}. It says:

\textit{Among competing hypotheses, the one with the fewest assumptions should be selected.}

Here, I identify hypotheses with models. Thus, this principle tells us we should choose the model with the fewest assumptions. I classify the assumptions of a model in two groups: fixed and free ones. The fixed assumptions belong to what I previously described as the prior information, $M$, used to construct the model. They may render the model more interpretable  in the physically or statistically sense,  or give it choerency within a corpus of hypotheses. Free assumptions correspond to the parameters of the model. They give it more flexibility to fit the data. For example, in the case of a straight line model, a fixed assumption is that the data is linearly related, whereas the free assumptions correspond to the slope and ordinate at the origin. A linear model and a quadratic model in which the constant term has been fixed, have the same number of free parameters (assumptions) but clearly the second one has an extra fixed assumption. Therefore, choosing the model with fewer free parameters does not necessarily means choosing the model with the fewest assumptions.

One of the great advantages of the Bayesian methodology is that it incorporates directly Ockham's principle. Suppose we want to compare two models $M_1$ and $M_2$, which we assume describe the data set $\mathbf{D}$. Each model has prior probabilities, $p(M_k)$ and likelihoods $p(\mathbf{D}|M_k)$ (with $k=1,2$). The prior probabilities reflect our believes about the fixed assumptions mentioned before. On the other hand, the likelihood of the data, given the model, is related parameters (the free assumptions) and priors within a model. It corresponds to the \emph{evidence} of the model, Eq. \ref{eq:evidence}. This evidence in terms of the model parameters, $\theta_k$, is

 \begin{equation}
p(\mathbf{D}|M_k)=\int_{\theta_k} p(\mathbf{D}|\theta_k,M_K)\cdot p(\theta_k|M_k)\cdot d\theta_k. \label{eq:evidence2}
\end{equation}
The Bayes' theorem applied to models instead of individual parameters tells us that
\begin{equation}
p(M_k|\mathbf{D})=\frac{p(\mathbf{D}|M_k)\cdot p(M_k)}{p(\mathbf{D})}.
\end{equation}
with $k=1,2$. Since there are only two models, their prior probabilities are related by $p(M_1)= 1- p(M_2)$. Therefore,
 \begin{equation}
p(M_k|\mathbf{D})=\frac{p(\mathbf{D}|M_k)\cdot p(M_k)}{p(\mathbf{D}|M_1)\cdot p(M_1)+p(\mathbf{D}|M_2)\cdot p(M_2)}.
\end{equation}
From this last Equation, the ratio of the posterior distributions is:
\begin{equation}
\frac{p(M_1|\mathbf{D})}{p(M_2|\mathbf{D})}=\frac{p(\mathbf{D}|M_1)\cdot p(M_1)}{p(\mathbf{D}|M_2)\cdot p(M_2)}.
\end{equation}
This ratio provides an objective measure of how better model $M_1$ is when compared to model $M_2$, under the measure provided by the data $\mathbf{D}$ by means of the evidence. When both prior probabilities  $p(M_1)$ and $p(M_2)$ are set equal, the ratio of posteriors equal the ratio of likelihoods. This is known as the \emph{Bayes factor}\footnote{For a similar derivation and some example see \citep{Kaas1995}}. Even in the equal priors case, the evidences themselves, Eq. \ref{eq:evidence2}, embody the Ockham's principle. Indeed, each evidence is a measure of the prior times the likelihood, this time for parameters in a single model. The larger the number of parameters (assumptions), the larger the volume, in parametric space, over which the likelihood of the data spreads. Since the likelihood is not a probability distribution on the parameters, it does not integrate to one, even if the priors are uniform. The evidence also penalises the assumptions made in the priors of the parameters. The most concentrated the prior is the less of the likelihood contributes to the evidence.

Thus, the Bayes' theorem is the way to update knowledge, either if it refers to models or to parameters within a model. 

\subsection{Membership probability}

In the previous Section, I derived, by means of the Bayes' theorem, the probability of models $M_1$ and $M_2$ given the data $\mathbf{D}$. Now, I describe the same problem but instead of the likelihood of a data set I do it for a single datum. This is, the probability of model $M_1$ or $M_2$, given the datum $\mathbf{d}$. This is known as the membership probability of the datum $\mathbf{d}$ to model or class, $M_k$ ($k=1,2$). The Bayes' theorem in this case is,

\begin{equation}
p( M_k | \mathbf{d}) =\frac{p(\mathbf{d}|M_k)\cdot p(M_k)}{\sum_{k=1}^2 p(\mathbf{d}|M_k)\cdot p(M_k)} 
\end{equation}

\section{Bayesian hierarchical Models}
\subsection{Generalities}
Bayesian formalism requires the establishment of priors. As mentioned before, priors represent the \emph{a priori} believe the user of the model has about the possible values that parameters of the model can take. This is indeed subjective. This subjectivity is the main source of criticism from the non-bayesian community \footnote{See \citep{Gelman2012} for a discussion on the ethical use of prior information}. 

Bayesian hierarchical models, in the following (BHM) can be grouped into the Empirical Bayes methods. In these later ones, the prior distributions are inferred from the data, rather than being directly specified as in common Bayesian methods. In BHM, priors are specified by parametric distributions whose parameters (called hyper-parameters) are also drawn from a parametric distribution in a hierarchical fashion. For this reason, hierarchical models are also called multilevel models. A fully-BHM is that in which the parameters at its higher hierarchy are drawn from a non-parametric distribution. Given its properties, BHM represent the most objective way to the establishment of prior distributions \citep{Gelman2006}. Despite the possible high hierarchy of a BHM, for it to be valid, the class of prior distribution must allow the \emph{true} value of its parameter \citep{Morris1983}. For this reason, the updating of knowledge is an important step in the any Bayesian study. If the posterior distribution is in total discrepancy with the prior distribution, or even worst, when the posterior is not fully allowed by the prior distribution (as in a truncated prior for example), then we must update our prior and allow the data to be fully expressed. Otherwise, the posterior could be biased.

Despite its theoretical advantages, BHM are difficult to evaluate since they require far more parameters than standard Bayes methods.
Furthermore, their hierarchy (levels) must stop at some point. There are at least two approaches to stop this hierarchy. The first one is to use a non parametric distribution for the hyper-parameters at the higher level. This renders, as previously said, a fully-BHM. Another alternative is to give a point estimate, usually the mean or the mode, for the distribution of the hyper-parameter at the top of the hierarchy.  

Although in BHM the parameters of the prior distributions are inferred from data, the user of the model has the important task of specifying the class of prior distribution. This has been an active area of research in late years.Common options are conjugate, non-informative, and weakly informative priors. Conjugate priors are those in which the posterior distribution turns out to be in the same family as the prior distribution. Non-informative and weakly informative priors, as they name indicates, provide intentionally weaker information or no information at all about the problem. Weakly informative prior are the recommended starting point compared to non-informative priors \cite[see for example the works of][]{Gelman2006,Huang2013,Chung2015}. Despite the kind of prior distribution chosen, we must always evaluate the prior distribution in terms of the posterior, to see if this last one make sense \citep{Gelman2006}.
\subsection{Examples}
Its applications in IA.
Its applications in astrophysics.
\subsection{Graphical representation:Probabilistic Graphical Models}

\section{Modelling the data}
Positions, proper motions, photometry
\subsection{Missing values}
\subsubsection{Missing value pattern}
\subsection{The field population}
\subsection{The cluster population}
\section{Priors}

\section{Sampling the posterior distribution}

Description of the techniques used to obtain samples from the posterior distribution.

History and used versions.

\subsection{PSO}
\subsubsection{The charged PSO}


\subsection{MCMC}
\subsubsection{Generalities}
\subsubsection{Flavours}
HMC,NUTS,Gibbs, Metropolis-Hasting, Affine invariant, stretch-move, MultiNest.
It must be clear why we choose emcee and multinest
\subsubsection{Convergence}
\subsubsection{The evidence}


\section{Codes}
\subsubsection{The modified charged PSO}
\subsubsection{Improvements of emcee}
\subsubsection{The GMM with missing values}
\subsection{Parallel implementations}
Description of the implementation. MPI, python stan, etc.
explain in detail the difficulties faced at implementing the different codes in the different servers.

